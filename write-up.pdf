<html><head><title>Gesture Recognition Project Write-up</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(249, 228, 188, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body class="pdf en-US" lang="en-US"><article id="abe47db6-30c9-4d25-8534-e2b32e5ff7e4" class="page sans"><header><h1 class="page-title">Gesture Recognition Project Write-up</h1><p class="page-description"></p></header><div class="page-body"><p id="950e1e5c-585c-4dbd-867e-3488c62831c4" class="">
</p><p id="761d6c36-33cc-4d7e-9fb7-8ba863f27f65" class="">This project focuses on building a model for gesture recognition using deep learning techniques. We explored several architectures to determine the most effective model for accurate gesture classification. Among the tested models, two stood out: Model 2 (Conv3D-based) and Model 5 (MobileNet V2 with GRU). Below is a detailed explanation of these models and the decision-making process behind choosing them.</p><h3 id="07f5df3c-12d8-4b7f-942a-d3d4f97976c8" class="">Model 2: Conv3D-based Architecture</h3><p id="d0b004ee-0009-4b9e-9f69-c9ae4e63e44a" class="">Model 2 uses four Conv3D layers followed by two fully connected layers and a softmax output layer. This architecture processes gesture sequences as volumes of frames, treating the temporal dimension similarly to how images are handled in CNNs.</p><h3 id="efe99eb4-df9e-432d-bc7b-d88270a4695b" class="">Architecture:</h3><ul id="6734ba3f-b5a1-45c9-8a74-c621cef5ee64" class="bulleted-list"><li style="list-style-type:disc"><strong>Convolutional Layers</strong>: Four Conv3D layers, which capture spatio-temporal information in the input sequences. Conv3D layers are particularly effective for extracting features from videos or sequences of frames.</li></ul><ul id="c02e8c27-0be5-4d04-9226-1e76caa4f9c7" class="bulleted-list"><li style="list-style-type:disc"><strong>Fully Connected Layers</strong>: Two dense layers with 128 neurons each. These layers help in learning non-linear combinations of the high-level features extracted by the Conv3D layers.</li></ul><ul id="b53b917f-5d3d-4e1c-9cb8-04f7067d651e" class="bulleted-list"><li style="list-style-type:disc"><strong>Activation Function</strong>: ReLU, chosen for its non-linear properties and ability to avoid vanishing gradients.</li></ul><ul id="c4ebe897-c46d-447d-8ab7-7325d07f118b" class="bulleted-list"><li style="list-style-type:disc"><strong>Output Layer</strong>: Softmax layer for multi-class classification.</li></ul><ul id="3aa1e1d8-d88d-4b3f-8885-d5c6ecacb541" class="bulleted-list"><li style="list-style-type:disc"><strong>Optimizer</strong>: Adam, with the default learning rate, was chosen for its fast convergence and adaptability to different gradients.</li></ul><h3 id="39e70316-d30f-4a20-913a-008277cd2634" class="">Hyperparameters:</h3><ul id="f5f065b5-513b-410d-9373-9e3482552997" class="bulleted-list"><li style="list-style-type:disc"><strong>Batch Size</strong>: 32 (reduced from 64). Reducing the batch size led to quicker updates to the weights, allowing for faster convergence in terms of epochs.</li></ul><ul id="63685651-e225-43e7-8ea7-aa770cbf1dd2" class="bulleted-list"><li style="list-style-type:disc"><strong>Number of Frames per Sequence</strong>: 30</li></ul><ul id="c5b813ed-7885-49a8-91fb-736734d9366d" class="bulleted-list"><li style="list-style-type:disc"><strong>Input Image Shape</strong>: (120, 120, 3)</li></ul><ul id="746841c4-d357-4830-bd5c-e451e1249f0e" class="bulleted-list"><li style="list-style-type:disc"><strong>Epochs</strong>: 20</li></ul><h3 id="f5cb0149-04b8-4081-a89b-6c43874ac501" class="">Performance:</h3><ul id="c4897524-33e3-45d3-a06f-e71383a936e0" class="bulleted-list"><li style="list-style-type:disc"><strong>Training Accuracy</strong>: 96.23%</li></ul><ul id="3e8dfc18-eef8-4d16-8d61-4e641d26286b" class="bulleted-list"><li style="list-style-type:disc"><strong>Validation Accuracy</strong>: 91.00%</li></ul><ul id="4ce143bc-1996-4c50-9688-8f147d96dfd8" class="bulleted-list"><li style="list-style-type:disc"><strong>Training Loss</strong>: 0.0990</li></ul><ul id="85182c82-6d84-4b79-94ac-b6b37bd0dd62" class="bulleted-list"><li style="list-style-type:disc"><strong>Validation Loss</strong>: 0.2845</li></ul><h3 id="48accaa5-4905-44a3-9386-1c7bebf93e54" class="">Insights:</h3><p id="a024a2e5-6848-4a82-9443-ee6d69912da1" class="">The decision to reduce the batch size from 64 to 32 was made after observing that smaller batch sizes led to faster weight updates. This helped Model 2 reach high accuracies quicker than its predecessors. The Conv3D architecture proved to be highly effective in capturing spatio-temporal features, as evidenced by the high validation accuracy of 91.00%, with minimal overfitting (as indicated by the low gap between training and validation accuracies).</p><p id="cfedbed7-4ba0-4368-bc4e-47acfaf7560c" class="">Model 2, with its Conv3D layers, was selected as the best performing model at this stage. It showcased a balanced performance in terms of both accuracy and loss, with little to no signs of overfitting, making it a strong candidate for further refinement.</p><h3 id="1aaabfde-330b-42c4-ab7b-efb7bfa69d3e" class="">Model 5: MobileNet V2 with GRU</h3><p id="845ea6f9-bcbd-48f4-93df-882308a819c6" class="">Model 5 introduces a more advanced architecture by leveraging MobileNet V2 as a pre-trained Conv2D model, combined with a GRU (Gated Recurrent Unit) layer. The key change here is the integration of a pre-trained model for feature extraction and a sequence processing unit (GRU) for temporal understanding.</p><h3 id="ca343bc7-ce5a-45e3-8d96-7b5d412458a0" class="">Architecture:</h3><ul id="cd46ce07-aec0-4e14-a033-ad5e94a329cd" class="bulleted-list"><li style="list-style-type:disc"><strong>MobileNet V2</strong>: Used as the backbone for feature extraction. MobileNet V2 is known for its efficiency and accuracy in image recognition tasks. In this model, we retrained the weights using our dataset to fine-tune it for gesture recognition.</li></ul><ul id="3fa9e4a1-88c9-4807-b103-4f0a3f3e659a" class="bulleted-list"><li style="list-style-type:disc"><strong>GRU Layer</strong>: GRU is a type of recurrent neural network (RNN) that is particularly useful for sequence prediction. It was added to capture the temporal dependencies across frames. The GRU layer contained 128 cells with a dropout of 0.5 to prevent overfitting.</li></ul><ul id="8827aa5a-e324-4498-81b3-38c9e1e15436" class="bulleted-list"><li style="list-style-type:disc"><strong>Fully Connected Layer</strong>: Increased the width of the fully connected layer to 256 neurons, allowing it to better capture the relationships between high-level features extracted by MobileNet and temporal data captured by the GRU.</li></ul><ul id="91a92216-4fe9-4ab3-a8af-42cf96a5fa64" class="bulleted-list"><li style="list-style-type:disc"><strong>Activation Function</strong>: ReLU</li></ul><ul id="230ae838-95e9-46ed-b7f6-63165ceb110f" class="bulleted-list"><li style="list-style-type:disc"><strong>Output Layer</strong>: Softmax layer for classification.</li></ul><ul id="b76603d5-5542-4987-bb83-f9ce2560b508" class="bulleted-list"><li style="list-style-type:disc"><strong>Optimizer</strong>: Adam, with a reduced learning rate of 0.0001.</li></ul><h3 id="a691577d-2d5c-4211-bee0-eeb0823b2313" class="">Modifications:</h3><ul id="4b9c3b5c-bef3-4f91-966e-eefb9a778d13" class="bulleted-list"><li style="list-style-type:disc"><strong>Learning Rate</strong>: Reduced further to 0.0001 to allow for finer updates to weights during training. This helps in preventing overfitting and ensuring the model converges to a more optimal solution.</li></ul><h3 id="0095f5d7-533f-4171-8fde-240760f08a4b" class="">Focus:</h3><p id="e35a8d31-9b08-4948-a844-d58c0d1eb8a8" class="">Model 5 focuses on using the pre-trained MobileNet V2 for spatial feature extraction and the GRU layer for temporal feature analysis. The combination of these two architectures (Conv2D and GRU) allows for a strong balance between efficient feature extraction and effective sequence learning.</p><h3 id="375352aa-7706-4fc5-bc45-e0228ccc6bf0" class="">Conclusion:</h3><p id="37cc9b36-338b-486a-ac21-17bf8cb63db8" class="">Model 2, with its Conv3D-based architecture, emerged as the best performing model in this project, achieving an excellent balance between training and validation accuracy with no signs of overfitting. The decision to reduce the batch size played a key role in achieving better results with this model. Meanwhile, Model 5 introduces a more complex architecture that leverages transfer learning and recurrent networks, and while it shows promise, its performance relative to Model 2 warrants further exploration.</p><p id="70cc37f5-0cbc-4012-8280-4bb176aa02d3" class="">In future iterations, more experimentation can be done, such as fine-tuning the GRU parameters, trying different pre-trained models, or incorporating ConvLSTM layers, which may lead to even better performance. The industry demo on CNNs provided a solid foundation for understanding the importance of experimenting with different data augmentation techniques, architectures, and hyperparameters to improve gesture recognition tasks.</p></div></article></body></html>